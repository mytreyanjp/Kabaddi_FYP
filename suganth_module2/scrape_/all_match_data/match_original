from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import time
import json
import re
#12 seasons match dropdown automate panna try pannathu
# Setup Chrome options
chrome_options = Options()
chrome_options.add_argument('--start-maximized')
chrome_options.add_argument('--disable-blink-features=AutomationControlled')

# Initialize driver
driver = webdriver.Chrome(options=chrome_options)

try:
    # Navigate to the schedule page
    print("Loading Pro Kabaddi schedule page...")
    driver.get("https://www.prokabaddi.com/schedule-fixtures-results")
    time.sleep(4)
    
    # Wait for filter section to load
    wait = WebDriverWait(driver, 10)
    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".filter-wrap")))
    
    all_seasons_data = []
    # seasons_to_scrape = ["Season 1", "Season 2", "Season 3", "Season 4", "Season 5", 
    #                      "Season 6", "Season 7", "Season 8", "Season 9", "Season 10", 
    #                      "Season 11", "Season 12"]
    seasons_to_scrape = ["Season 1"]     
    
    for target_season in seasons_to_scrape:
        print(f"\n{'='*60}")
        print(f"Processing {target_season}...")
        print(f"{'='*60}")
        
        try:
            # Click on season dropdown
            print("Clicking season dropdown...")
            dropdown_button = wait.until(EC.element_to_be_clickable(
                (By.CSS_SELECTOR, ".filter-wrap .waf-select-box:nth-child(1) .selected-title")
            ))
            driver.execute_script("arguments[0].click();", dropdown_button)
            time.sleep(1)
            
            # Wait for season buttons to be visible
            print("Waiting for season buttons...")
            wait.until(EC.presence_of_all_elements_located(
                (By.CSS_SELECTOR, ".filter-wrap .waf-select-box:nth-child(1) .list-item")
            ))
            
            # Find all season buttons
            season_buttons = driver.find_elements(By.CSS_SELECTOR, 
                ".filter-wrap .waf-select-box:nth-child(1) .list-item")
            
            print(f"Found {len(season_buttons)} season buttons")
            
            # Find and click the target season button
            season_found = False
            for button in season_buttons:
                if target_season in button.text:
                    print(f"Found {target_season} button, clicking...")
                    driver.execute_script("arguments[0].scrollIntoView(true);", button)
                    time.sleep(0.5)
                    try:
                        button.click()
                    except:
                        driver.execute_script("arguments[0].click();", button)
                    season_found = True
                    break
            
            if not season_found:
                print(f"ERROR: Could not find button for {target_season}")
                continue
            
            # Wait for season title to update and verify change
            print("Waiting for season change...")
            try:
                wait.until(EC.text_to_be_present_in_element(
                    (By.CSS_SELECTOR, ".filter-wrap .waf-select-box:nth-child(1) .selected-title .title"),
                    target_season
                ))
            except:
                print("WARNING: Season title verification timed out")
            
            # Additional wait for content to reload
            time.sleep(5)
            
            # Verify season change
            season_title = driver.find_element(By.CSS_SELECTOR, 
                ".filter-wrap .waf-select-box:nth-child(1) .selected-title .title").text
            print(f"Current season showing: {season_title}")
            
            if target_season not in season_title:
                print(f"ERROR: Season did not change to {target_season}. Still showing {season_title}")
                continue
            
            # Click on "Recent" tab to see completed matches with scores
            print("Clicking 'Recent' tab...")
            try:
                # Try multiple methods to find and click the Recent tab
                try:
                    recent_tab = driver.find_element(By.XPATH, "//div[contains(text(), 'Recent')]")
                    driver.execute_script("arguments[0].click();", recent_tab)
                except:
                    recent_tab = driver.find_element(By.CSS_SELECTOR, "[id*='tab3']")
                    driver.execute_script("arguments[0].click();", recent_tab)
                time.sleep(3)
            except Exception as e:
                print(f"Could not click Recent tab: {e}")
            
            # Scroll to bottom to load all matches
            print("Scrolling to load all matches...")
            last_height = driver.execute_script("return document.body.scrollHeight")
            scroll_attempts = 0
            max_scrolls = 15
            
            while scroll_attempts < max_scrolls:
                # Scroll down
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(3)
                
                # Calculate new height
                new_height = driver.execute_script("return document.body.scrollHeight")
                
                if new_height == last_height:
                    break
                
                last_height = new_height
                scroll_attempts += 1
                print(f"Scroll attempt {scroll_attempts}/{max_scrolls}")
            
            print("Finished scrolling, extracting data...")
            
            # Get page source
            page_source = driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # Extract match data
            matches_list = []
            
            # Find all match links
            match_links = soup.find_all('a', href=re.compile(r'/matchcentre/\d+-scorecard'))
            
            print(f"Found {len(match_links)} total match links")
            
            for match_link in match_links:
                try:
                    # Extract match URL and ID
                    match_url = "https://www.prokabaddi.com" + match_link['href']
                    match_id_search = re.search(r'/matchcentre/(\d+)-scorecard', match_link['href'])
                    match_id = match_id_search.group(1) if match_id_search else ""
                    
                    # Navigate up to find the date heading for this match
                    date_text = ""
                    parent = match_link.find_parent()
                    while parent:
                        date_heading = parent.find_previous(['h2', 'h3'])
                        if date_heading:
                            date_candidate = date_heading.get_text(strip=True)
                            if any(month in date_candidate for month in ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']):
                                date_text = date_candidate
                                break
                        parent = parent.parent
                        if not parent or parent.name == 'body':
                            break
                    
                    # Extract all text content from the link
                    all_text = match_link.get_text(separator='|', strip=True)
                    
                    # Split by the separator to get individual pieces
                    parts = [p.strip() for p in all_text.split('|') if p.strip()]
                    
                    # Initialize variables
                    team_a_name = ""
                    team_a_score = ""
                    team_b_name = ""
                    team_b_score = ""
                    status = "FT"
                    
                    # Parse parts to identify team names, scores, and status
                    team_names = []
                    scores = []
                    
                    for part in parts:
                        if part.isdigit() and len(part) <= 3:  # Score
                            scores.append(part)
                        elif part in ['FT', 'HT', 'LIVE']:  # Status
                            status = part
                        elif len(part) > 3 and not part.isdigit():  # Team name
                            team_names.append(part)
                    
                    # Assign teams and scores
                    if len(team_names) >= 2:
                        team_a_name = team_names[0]
                        team_b_name = team_names[1]
                    if len(scores) >= 2:
                        team_a_score = scores[0]
                        team_b_score = scores[1]
                    
                    # --- Match type extraction ---
                    match_type = ""
                    link_parent = match_link.find_parent()
                    keywords = ['Eliminator', 'Qualifier', 'Final', 'Semi', 'Play-in', 'Mini']
                    match_number_pattern = re.compile(r'Match\s*\d+', re.IGNORECASE)

                    if link_parent:
                        # Try to find <p class="match-count"> in previous siblings
                        for sib in link_parent.find_previous_siblings():
                            p_match_count = sib.find('p', class_='match-count')
                            if p_match_count:
                                match_type_text = p_match_count.get_text(strip=True)
                                if any(kw in match_type_text for kw in keywords) or match_number_pattern.search(match_type_text):
                                    match_type = match_type_text
                                    break
                            # Fallback: check the sibling's own text, sometimes match type is directly text of the div
                            sib_text = sib.get_text(strip=True)
                            if any(kw in sib_text for kw in keywords) or match_number_pattern.search(sib_text):
                                match_type = sib_text
                                break
                    # Absolute fallback: search backwards from the match_link itself
                    if not match_type:
                        prev = match_link.find_previous(['div', 'p', 'span'])
                        while prev:
                            prev_text = prev.get_text(strip=True)
                            if any(kw in prev_text for kw in keywords) or match_number_pattern.search(prev_text):
                                match_type = prev_text
                                break
                            prev = prev.find_previous(['div', 'p', 'span'])
                    
                    # Only add if we have minimum required data
                    if team_a_name and team_b_name:
                        match_data = {
                            "date": date_text,
                            "match_id": match_id,
                            "match_url": match_url,
                            "match_type": match_type,
                            "team_a": {
                                "name": team_a_name,
                                "score": team_a_score
                            },
                            "team_b": {
                                "name": team_b_name,
                                "score": team_b_score
                            },
                            "status": status
                        }
                        
                        matches_list.append(match_data)
                        print(f"  Extracted: {team_a_name} vs {team_b_name}")
                        
                except Exception as e:
                    print(f"  ERROR extracting match: {str(e)}")
                    continue
            
            # Create season data object
            season_data = {
                "season": target_season,
                "total_matches": len(matches_list),
                "matches": matches_list
            }
            
            all_seasons_data.append(season_data)
            
            print(f"\nSummary for {target_season}:")
            print(f"  Total matches extracted: {len(matches_list)}")
            if len(matches_list) > 0:
                print(f"  Sample: {matches_list[0]['team_a']['name']} vs {matches_list[0]['team_b']['name']}")
            
        except Exception as e:
            print(f"ERROR processing {target_season}: {str(e)}")
            import traceback
            traceback.print_exc()
            continue
        
        time.sleep(2)
    
    # Save all data to JSON file
    output_file = "prokabaddi_matches.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_seasons_data, f, indent=2, ensure_ascii=False)
    
    print(f"\n{'='*60}")
    print(f"SCRAPING COMPLETE!")
    print(f"{'='*60}")
    print(f"Total seasons scraped: {len(all_seasons_data)}")
    print(f"Data saved to: {output_file}")
    
    # Print summary
    print("\nSummary by season:")
    for season_data in all_seasons_data:
        print(f"  {season_data['season']}: {season_data['total_matches']} matches")
    
    # Show sample data
    if all_seasons_data and all_seasons_data[0]['matches']:
        print("\nSample match data:")
        print(json.dumps(all_seasons_data[0]['matches'][0], indent=2))

finally:
    print("\nClosing browser...")
    driver.quit()
    print("Done!")
